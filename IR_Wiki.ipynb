{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+ Get entities from wikipedia\n",
    "#+ Get Pages from those entities\n",
    "#+ Get abstract of whose pages\n",
    "#+ Split abstact on passages (in wikipedia spliters can be  == Movement to Organize Black Psychologists == , or \\n)\n",
    "# use word2vec for comare passage and question\n",
    "# return 1 most relevant passage to aqustion for page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree \n",
    "import tagme\n",
    "from operator import itemgetter\n",
    "import wikipedia\n",
    "wikipedia.set_lang(\"en\")\n",
    "tagme.GCUBE_TOKEN = \"c294fe63-88fa-40e1-b1d4-34b70c887b29-843339462\"\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import nltk, string\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class text_classifier:\n",
    "    # can be improved by using word vectors\n",
    "    def __init__(self, data, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.texts_vectors = self.vectorizer.vectorize_data(data)\n",
    "        self.texts=data\n",
    "        \n",
    "    def get_cosines_index(self, text):\n",
    "        #vectoriztion\n",
    "        text_vector = self.vectorizer.vectorize_text(text)\n",
    "        #dot similarity\n",
    "        cosine = cosine_similarity(text_vector, self.texts_vectors)\n",
    "        return cosine[0].argsort()[:-1]\n",
    "\n",
    "    \n",
    "    def get_jaccard_index(self, text):\n",
    "        #vectoriztion\n",
    "        distances = []\n",
    "        for source_text in self.texts:\n",
    "            distances.append(self.DistJaccard(text,source_text))\n",
    "        return np.argsort(distances)[:-1]\n",
    "    \n",
    "    def DistJaccard(self, str1, str2):\n",
    "        str1 = set(self.vectorizer.tokenize(str1))\n",
    "        str2 = set(self.vectorizer.tokenize(str2))\n",
    "        return len(str1 & str2)\n",
    "    \n",
    "    \n",
    "class english_text_vectorizer:\n",
    "    \n",
    "    def __init__(self, folder_with_vectorizers, en_nlp):\n",
    "        self.folder_with_vectorizers=folder_with_vectorizers\n",
    "        self.en_nlp = en_nlp\n",
    "        self.vectorizer = HashingVectorizer(tokenizer=lambda text:self.get_lemmas(text),n_features=100000)\n",
    "        self.remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "        \n",
    "        try:\n",
    "            self.vect = joblib.load(folder_with_vectorizers+'en_HashingVectorizer.pkl')\n",
    "            self.vect.tokenizer=self.text_to_lemmas\n",
    "        except:\n",
    "            print(\"HashingVectorizer was not loaded\")\n",
    "            self.vect = HashingVectorizer(tokenizer=lambda text:self.tokenize(text),n_features=100000)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower().translate(self.remove_punctuation_map)\n",
    "        return [token.lemma_ for token in self.en_nlp(text)]\n",
    "        \n",
    "    def vectorize_data(self, texts):\n",
    "        try:\n",
    "            return self.vect.transform(texts)\n",
    "        except:\n",
    "            vectors = self.vect.fit_transform(texts)\n",
    "            joblib.dump(self.vect, self.folder_with_vectorizers+'en_HashingVectorizer.pkl', compress=9)\n",
    "            return vectors        \n",
    "    \n",
    "    def vectorize_text(self, text):\n",
    "        return self.vect.transform([text])[0]\n",
    "\n",
    "class cache_helper:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.dict = None  \n",
    "    \n",
    "    def get(self, key, get_value):\n",
    "        if self.dict == None:\n",
    "            self.load()\n",
    "        if key not in self.dict:\n",
    "            print('need to download')\n",
    "            self.set_item(key, get_value(key))\n",
    "        return self.dict[key]   \n",
    "    \n",
    "    def set_item(self, key, value):\n",
    "        self.dict[key]=value\n",
    "        self.save()\n",
    "        \n",
    "    def save(self):\n",
    "        with open(self.file_path, 'wb') as f:\n",
    "            pickle.dump(self.dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'rb') as f:\n",
    "                self.dict=pickle.load(f)\n",
    "        except:\n",
    "            self.dict={}\n",
    "    \n",
    "class WikiHelper:\n",
    "    \n",
    "    def __init__(self, en_nlp, cache_file):\n",
    "        self.en_nlp=en_nlp\n",
    "        self.data=cache_helper(cache_file)\n",
    "        \n",
    "    def get_spacy_entities(self, text, count=3):\n",
    "        nlp_doc = self.en_nlp(text)\n",
    "        return [ent.text for ent in nlp_doc.ents[:count]]\n",
    "    \n",
    "    def get_wiki_entities(self, text, count=3):\n",
    "        newlist = [{'score': item.score,'mention':item.mention,'entity_title':item.entity_title} for item in tagme.annotate(text).get_annotations(0.1)]\n",
    "        newlist = sorted(newlist, key=itemgetter('score'), reverse=True)\n",
    "        return [{'mention':item['mention'],'entity_title':item['entity_title']} for item in newlist[:count]]\n",
    "    \n",
    "    def get_merget_entities(self, text):\n",
    "        entities = self.get_wiki_entities(text)\n",
    "        if len(entities)<2:\n",
    "            spacy_entities =  self.get_spacy_entities(text)\n",
    "            for spacy_entity in spacy_entities:\n",
    "                for entity in entities:\n",
    "                    if (spacy_entity not in entity['mention']) and (entity['mention'] not in spacy_entity):\n",
    "                        entities.append({'mention':spacy_entity,'entity_title':spacy_entity})\n",
    "        return entities\n",
    "    \n",
    "    def get_wiki_content(self, entity, pages_count):\n",
    "        fined_page_names = wikipedia.search(entity)\n",
    "        if len(fined_page_names)>0:\n",
    "            try:\n",
    "                content=[]\n",
    "                for page_name in fined_page_names[:pages_count]:\n",
    "                    wiki_page = wikipedia.page(page_name)\n",
    "                    content.append(wiki_page.content)\n",
    "                return '. '.join(content)\n",
    "            except:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    def get_pages(self, text):\n",
    "        print('wiki processing')\n",
    "        wiki_content=[]\n",
    "        entities = self.get_merget_entities(text)\n",
    "        for entity in entities:\n",
    "            wiki_content.append(self.get_wiki_content(entity['entity_title'],1))\n",
    "        return wiki_content\n",
    "    \n",
    "    def get_data(self,text):\n",
    "        texts = self.data.get(text, self.get_pages)\n",
    "        return [self.split_text_to_passages(text) for text in texts]\n",
    "    \n",
    "    def split_text_to_passages(self, text):\n",
    "        split_pattern = \"(\\\\n)+|(=+ ([a-zA-Z]* )+=+)\"\n",
    "        return [passage for passage in  re.split(split_pattern, text) if (passage != None) and (len(passage.strip().split(' '))>1) and (passage != '\\n') and (passage[0] != '=')]\n",
    "    \n",
    "    \n",
    "class IR_Wiki:\n",
    "    def __init__(self, en_nlp):\n",
    "        self.vectorizer=english_text_vectorizer('vectorizers',en_nlp)\n",
    "        self.wikiHelper = WikiHelper(en_nlp, 'WikiHelper.pkl')\n",
    "        \n",
    "    def get_most_relevant_passage(self, question, passages):\n",
    "        classifier = text_classifier(passages, self.vectorizer)\n",
    "        indexes = classifier.get_jaccard_index(question)        \n",
    "        return np.array(passages)[indexes]\n",
    "        \n",
    "    def get_passage(self, question):\n",
    "        data = self.wikiHelper.get_data(question)\n",
    "        passages = np.concatenate(data)      \n",
    "        return self.get_most_relevant_passage(question,passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingVectorizer was not loaded\n"
     ]
    }
   ],
   "source": [
    "ir = IR_Wiki(en_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas = ir.get_passage('what is the capital of ukraine ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Hospitals in Ukraine are organised along the same lines as most European nations, according to the regional administrative structure; as a result most towns have their own hospital (Міська Лікарня) and many also have district hospitals (Районна Лікарня). Larger and more specialised medical complexes tend only to be found in major cities, with some even more specialised units located only in the capital, Kiev. However, all oblasts have their own network of general hospitals which are able to deal with almost all medical problems and are typically equipped with major trauma centres; such hospitals are called 'regional hospitals' (Обласна Лікарня).\",\n",
       "       \"In 1934, the capital of Soviet Ukraine moved from Kharkiv to Kiev. Previously, the city was seen as only a regional centre, hence received little attention. All of that was to change, at great price. The first examples of Stalinist architecture were already showing, and, in light of the official policy, a new city was to be built on top of the old one. This meant that much-admired examples such as the St. Michael's Golden-Domed Monastery were destroyed. Even the St. Sophia Cathedral was under threat. Also, the Second World War contributed to the wreckage. After the war, a new project for the reconstruction of central Kiev transformed Khreshchatyk avenue into a notable example of Stalinism in Architecture. However, by 1955, the new politics of architecture once again stopped the project from fully being realised.\",\n",
       "       \"In the mid-14th century, upon the death of Bolesław Jerzy II of Mazovia, king Casimir III of Poland initiated campaigns (1340–1366) to take Galicia-Volhynia. Meanwhile, the heartland of Rus', including Kiev, became the territory of the Grand Duchy of Lithuania, ruled by Gediminas and his successors, after the Battle on the Irpen' River. Following the 1386 Union of Krewo, a dynastic union between Poland and Lithuania, much of what became northern Ukraine was ruled by the increasingly Slavicised local Lithuanian nobles as part of the Grand Duchy of Lithuania. By 1392 the so-called Galicia–Volhynia Wars ended. Polish colonisers of depopulated lands in northern and central Ukraine founded or re-founded many towns. In 1430 Podolia was incorporated under the Crown of the Kingdom of Poland as Podolian Voivodeship. In 1441, in the southern Ukraine, especially Crimea and surrounding steppes, Genghisid prince Haci I Giray founded the Crimean Khanate.\",\n",
       "       'In the 5th and 6th centuries, the Antes were located in the territory of what is now Ukraine. The Antes were the ancestors of Ukrainians: White Croats, Severians, Polans, Drevlyans, Dulebes, Ulichians, and Tiverians. Migrations from Ukraine throughout the Balkans established many Southern Slavic nations. Northern migrations, reaching almost to the Ilmen Lakes, led to the emergence of the Ilmen Slavs, Krivichs, and Radimichs, the groups ancestral to the Russians. After an Avar raid in 602 and the collapse of the Antes Union, most of these peoples survived as separate tribes until the beginning of the second millennium.'],\n",
       "      dtype='<U2436')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test performanse based on passages from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('passages_test_query', 'rb') as fp:\n",
    "    passages_test_query = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'average cost to build large retaining wall'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_test_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('passages_test', 'rb') as fp:\n",
    "    passages_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_passages_and_label(passage_group):\n",
    "    passages=[]\n",
    "    label = None\n",
    "    for passage in passage_group:\n",
    "        passages.append(passage['passage_text'])\n",
    "        if (passage['is_selected']):\n",
    "            label=passage['passage_text']\n",
    "    return passages, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages, correct_passage = select_passages_and_label(passages_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 Materials and installation for a stone retaining wall start around $25-$40 a square face foot and go up depending on materials, site conditions, and other factors. 2  It costs $1,250-$2,000 or more for a wall 30 inches high and 20 feet long.hopping for a retaining wall: 1  Hiring an engineer to help design a retaining wall can run about $200-$500 or more, depending on the complexity and extent of the project. 2  Hiring an engineer is strongly recommended for walls over 3 feet high.',\n",
       "       \"1 When the wall is increased to 8' in height the cost will average at $13,700, and when the height is bumped up to 10' the cost increases to $19,900.2  Estimates and quotes-like many other major construction projects it is advisable to seek out estimates from a range of providers.or the purpose of this discussion we will consider the costs of building a retaining wall from cast concrete and at a length of 50 linear feet. We will also take into consideration various heights as well, ranging from six, eight and ten feet.\"],\n",
       "      dtype='<U694')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(passages)[[1,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cortana what's agony :  [False False False False False False False False False]\n",
      "+++++++++++++++++++++\n",
      "None\n",
      "+++++++++++++++++++++\n",
      "noun, plural ironies. 1. the use of words to convey a meaning that is the opposite of its literal meaning: the irony of her reply, “How nice!” when I said I had to work all weekend. 2. Literature. a technique of indicating, as through character or plot development, an intention or attitude opposite to that which is actually or ostensibly stated.\n",
      "---------------------\n",
      "Micropenis: When a Penis Really Is Too Small. There is, of course, such a thing as a very small penis. The medical term micropenis applies to the 0.6% of men with the smallest penises. According to Palmer's statistics, an SPL of three and two-thirds inches or less indicates a micropenis.\n",
      "---------------------\n",
      "Boston crab. Chris Jericho with his Walls of Jericho (Elevated Boston crab) on Shawn Michaels. The Boston crab is a professional wrestling hold that typically starts with one wrestler lying in a supine position on the mat, with the other wrestler standing and facing him. It is a type of spinal lock where the wrestler hooks each of the opponent’s legs in one of his arms, and then turns the opponent face-down, stepping over him in the process.\n",
      "---------------------\n",
      "Irony vs Sarcasm. Irony and sarcasm are often confused, which is understandable. In some cases, they are interchangeable. This is because sarcasm is a kind of irony, so all instances of sarcasm are irony, but not all instances of irony are sarcasm. Irony is when something appears to be or is said to be one way, but is actually another.\n",
      "---------------------\n",
      "I had my first date around 1971 with a high school sweethart. At that time first base was your first date. Just fondling and dry kissing, wet kissing was considered disgusting, no clothing was taken off. Second base included feeling each other more, she let me feel her tits through her blouse. Still no clothing was removed.\n",
      "---------------------\n",
      "a popular political assembly. 2. the place where such an assembly met, originally a marketplace or public square. 3. the Agora, the chief marketplace of Athens, center of the city's civic life. Origin of agora1.\n",
      "---------------------\n",
      "But behind the closed doors of a doctor's examining room, it's a common question. Pediatric urologist Lane S. Palmer, MD, chief of pediatric urology at Cohen Children's Medical Center, New Hyde Park, N.Y., hears it a lot.\n",
      "---------------------\n",
      "The Gravemind uses a line from Halo 2: We exist together now. Two corpses in one grave.. This line is referring to Cortana, and referred to the Prophet of Mercy in Halo 2. This line was said in the level High Charity..\n",
      "---------------------\n",
      "Christina Aguilera Face. The agony and the Xtinasy of Christina Aguilera’s contorted face. This is what a stroke looks like in super slo-mo. Gavon Laessig.\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "\n",
    "passages, correct_passage = select_passages_and_label(passages_test[index])\n",
    "predicted_passage = ir.get_most_relevant_passage(passages_test_query[index], passages)\n",
    "print(passages_test_query[index],': ',correct_passage==predicted_passage)\n",
    "print('+++++++++++++++++++++')\n",
    "print(correct_passage)\n",
    "print('+++++++++++++++++++++')\n",
    "for p in predicted_passage:\n",
    "    print(p)\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there quite similar passages with information, but machine comprehension model somehow select from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passages_test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:44<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    passages, correct_passage = select_passages_and_label(passages_test[i])\n",
    "    predicted_passage = ir.get_most_relevant_passage(passages_test_query[i], passages)\n",
    "    #print(list(correct_passage==predicted_passage))\n",
    "    try:\n",
    "        results.append(list(correct_passage==predicted_passage).index(True))\n",
    "    except:\n",
    "        results.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 532,\n",
       "         0: 32,\n",
       "         1: 31,\n",
       "         2: 34,\n",
       "         3: 34,\n",
       "         4: 52,\n",
       "         5: 50,\n",
       "         6: 76,\n",
       "         7: 69,\n",
       "         8: 90})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
